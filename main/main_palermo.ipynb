{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Try:\n",
    "* Drop all zero electricity meter readings\n",
    "* Explore and possibly remove building 1099\n",
    "* Try cross validation and ensembling of models\n",
    "* Treat categorical missing / NaNs\n",
    "\n",
    "\n",
    "* X Remove noisy days for site 0 (See Strategy...notebook on Kaggle)\n",
    "* X Create time series visualization by site / building / meter type\n",
    "* X Use meter type as a feature ({0: electricity, 1: chilledwater, 2: steam, 3: hotwater})\n",
    "* X Add building and site id features (see https://www.kaggle.com/aitude/ashrae-kfold-lightgbm-without-leak-1-08)\n",
    "    * Set categorical dataset in lgbm fit\n",
    "* X Research validation strategy and implement\n",
    "* X 'Primary use' indicator\n",
    "* X Additional datebased features (month and quarterly indicators, time trends)\n",
    "* X LightGBM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import feather\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN = pathlib.Path('/Users/palermopenano/personal/kaggle_energy')\n",
    "SUBMISSIONS_PATH = MAIN / 'submissions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = False\n",
    "train_full = True         # False to do KFold CV, True to train on full dataset\n",
    "create_submission = True  # True to generate submission csv on test\n",
    "\n",
    "# Number of hours to compute moving average\n",
    "period = 3\n",
    "# period = 24\n",
    "\n",
    "submission_name = 'submission_2019-11-29_remove_zero_electricity_meter_readings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNC (does not change)\n",
    "train = pd.read_csv(MAIN / 'data' / 'train.csv')\n",
    "train['timestamp'] = pd.to_datetime(train['timestamp'], infer_datetime_format=True)\n",
    "\n",
    "building_metadata = pd.read_csv(MAIN / 'data' / 'building_metadata.csv')\n",
    "\n",
    "weather_train = pd.read_csv(MAIN / 'data' / 'weather_train.csv')\n",
    "weather_train['timestamp'] = pd.to_datetime(weather_train['timestamp'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# df = train\n",
    "# df = df.loc[(df.building_id == 1099) & (df.meter == 0)]\n",
    "# sns.lineplot(x='timestamp', y='meter_reading',data=df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute rolling stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!\n",
    "# Rolling statistic for weather data\n",
    "cols_rol = [\n",
    "    'air_temperature', \n",
    "    'dew_temperature',\n",
    "    'sea_level_pressure',\n",
    "    'wind_speed'\n",
    "]\n",
    "\n",
    "tmp = rolling_stat(\n",
    "    weather_train, 'timestamp', ['site_id'], \n",
    "    cols_rol, period, np.mean\n",
    ")\n",
    "weather_train = weather_train.drop(cols_rol, 1)\n",
    "weather_train = weather_train.merge(tmp, how='inner', on=['site_id', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only a random sample of n buildings\n",
    "if sample:\n",
    "    train, randbuilding = df_sample_random_buildings(train, 'building_id', n=5)\n",
    "    print(randbuilding)\n",
    "\n",
    "# train = train[train.meter == 0]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNC\n",
    "train = train.merge(building_metadata, on='building_id', how='left')\n",
    "train = train.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "print(\n",
    "    f\"Min time {train['timestamp'].min()}\",\n",
    "    f\"Max time {train['timestamp'].max()}\",\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce memory usage\n",
    "train = reduce_mem_usage(train, cols_exclude=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  first 141 days of meter=0 for site 0\n",
    "* See discussions in https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113054#latest-675811\n",
    "* No changes need to be made to the test set as this only concerns the meter_reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before filter:\", train.shape)\n",
    "first141d_site0_meter0_cond = \\\n",
    "    (train.site_id == 0) & \\\n",
    "    (train.meter == 0) & \\\n",
    "    (train.timestamp.dt.dayofyear >= 0) & \\\n",
    "    (train.timestamp.dt.dayofyear <= 141)\n",
    "\n",
    "train = train.loc[~first141d_site0_meter0_cond,:]\n",
    "print(\"After filter:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration code\n",
    "\n",
    "# df = train.loc[\n",
    "#     (train.site_id == 0) & (train.meter == 0), \n",
    "#     ['building_id','meter','timestamp','meter_reading']]\n",
    "# df = df.sort_values('timestamp', ascending=True)\n",
    "# bs_site0 = df.building_id.unique()\n",
    "# daysofyear = df.timestamp.dt.dayofyear.unique()\n",
    "\n",
    "# for d in daysofyear:\n",
    "#     cond = df.timestamp.dt.dayofyear == d\n",
    "# #     print(df.loc[cond, 'meter_reading'].head())\n",
    "#     print(d, df.loc[cond, 'meter_reading'].median(), df.loc[cond, 'meter_reading'].mean())\n",
    "\n",
    "# print(bs_site0)\n",
    "# for b in bs_site0:\n",
    "#     cond = (df.timestamp.dt.dayofyear >= 0) & \\\n",
    "#            (df.timestamp.dt.dayofyear <= 141) & \\\n",
    "#            (df.building_id == b)\n",
    "    \n",
    "#     median = df.loc[cond, 'meter_reading'].median()\n",
    "#     mean = df.loc[cond, 'meter_reading'].mean()\n",
    "#     print(b, median, mean)\n",
    "\n",
    "# import seaborn as sns\n",
    "# for b in bs_site0:\n",
    "#     print(b)\n",
    "#     cond = (df.timestamp.dt.dayofyear >= 0) & \\\n",
    "#            (df.timestamp.dt.dayofyear <= 141) & \\\n",
    "#            (df.building_id == b)\n",
    "#     sns.lineplot(x='timestamp',y='meter_reading',data=df[cond])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all zero or missing electricity meter readings\n",
    "print(\"Before remove zero electiricity meter reading filter:\", train.shape)\n",
    "zero_meter_reading_cond = \\\n",
    "    (train.meter == 0) & \\\n",
    "    (train.meter_reading == 0.0)\n",
    "train = train.loc[~zero_meter_reading_cond, :]\n",
    "print(\"After remove zero electiricity meter reading filter:\", train.shape)\n",
    "print(\"--------------\")\n",
    "print(\"Before remove na filter:\", train.shape)\n",
    "train = train.loc[~train.meter_reading.isna(), :]\n",
    "print(\"After remove na filter:\", train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: take log of square_feet\n",
    "train['square_feet'] = np.log1p(train['square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Add datebased features\n",
    "# Monday is 0\n",
    "# If dayofweek is 5 or 6, then it is a weekend\n",
    "# // is \"floored\" division (i.e. 6//5 is equal to 1, 3//5 is 0)\n",
    "\n",
    "add_datepart(\n",
    "    train, 'timestamp', datetimeformat=None,\n",
    "    drop=False, time=True, errors=\"raise\"\n",
    ")\n",
    "train[\"weekend\"] = train[\"timestamp\"].dt.weekday // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: precip_depth_1\n",
    "# Convert -1 and NaN precipitation to 0\n",
    "# Create trace rain indicator\n",
    "# Create NaN indicator\n",
    "\n",
    "def precip_depth_1_hr_FE(df, m):\n",
    "    df['precip_depth_1_hr_nan'] = df['precip_depth_1_hr'].isna()\n",
    "    \n",
    "    if m:\n",
    "        df.loc[df['precip_depth_1_hr'].isna(), 'precip_depth_1_hr'] = m\n",
    "    else:\n",
    "        m = df['precip_depth_1_hr'].median()\n",
    "        df.loc[df['precip_depth_1_hr'].isna(), 'precip_depth_1_hr'] = m\n",
    "\n",
    "    df['precip_depth_1_hr_isTrace'] = (df['precip_depth_1_hr'] == -1)\n",
    "    df.loc[df['precip_depth_1_hr'] == -1, 'precip_depth_1_hr'] = 0\n",
    "    return df, m\n",
    "\n",
    "train, precip_m = precip_depth_1_hr_FE(train, m=None)\n",
    "# train[['precip_depth_1_hr_nan', 'precip_depth_1_hr_isTrace', 'precip_depth_1_hr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: wind_direction\n",
    "# Replace nan with median wind_directin angle\n",
    "# Create nan indicator\n",
    "# Convert to sine and cosine features\n",
    "\n",
    "def wind_direction_FE(df, m=None):\n",
    "    df['wind_direction_nan'] = df['wind_direction'].isna()\n",
    "\n",
    "    if m:\n",
    "        df.loc[df['wind_direction'].isna(), 'wind_direction'] = m\n",
    "    else:\n",
    "        m = df['wind_direction'].median()\n",
    "        df.loc[df['wind_direction'].isna(), 'wind_direction'] = m\n",
    "\n",
    "    df['wind_direction_sin'] = np.sin(np.radians(df['wind_direction']))\n",
    "    df['wind_direction_cos'] = np.cos(np.radians(df['wind_direction']))\n",
    "    return df, m\n",
    "\n",
    "train, wind_direction_m = wind_direction_FE(train, m=None)\n",
    "# train[['wind_direction_nan','wind_direction_sin','wind_direction_cos','wind_direction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: primary_use\n",
    "# Apply label encoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['primary_use'] = le.fit_transform(train['primary_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Meter Reading (outcome variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNC\n",
    "train['meter_reading'] = np.where(\n",
    "    train['meter_reading']>=0, train['meter_reading'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth out time series moving average\n",
    "\n",
    "cols_rol_y = ['meter_reading']\n",
    "tmp = rolling_stat(\n",
    "    train, 'timestamp', ['building_id', 'meter'], \n",
    "    cols_rol_y, period, np.mean\n",
    ")\n",
    "\n",
    "train = train.drop(cols_rol_y, 1)\n",
    "train = train.merge(tmp, how='inner', on=['building_id', 'meter', 'timestamp'])\n",
    "\n",
    "# Shift back by an hour because moving average tends to shift the series forward\n",
    "train['meter_reading'] = train.groupby(['building_id','meter'])['meter_reading'].shift(-1)\n",
    "\n",
    "# Replace missing with mean for building / site\n",
    "train['meter_reading'] = (\n",
    "    train.\n",
    "    groupby(['building_id', 'meter'])['meter_reading'].\n",
    "    transform(lambda x: x.fillna(x.mean()))   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train['meter_reading'])\n",
    "print(y.ndim, y.shape, y.min(), y.max())\n",
    "print(y.describe().apply(lambda x: format(x, ',.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# dd = train.loc[\n",
    "#     (train.building_id == 544) & \n",
    "#     (train.meter == 0)\n",
    "# ]\n",
    "# d1 = dd.loc[\n",
    "#     (dd.timestamp >= '2016-10-08') & \n",
    "#     (dd.timestamp <= '2016-10-10'), \n",
    "#     ['meter_reading', 'timestamp']]\n",
    "\n",
    "# d2 = dd.loc[\n",
    "#     (dd.timestamp >= '2016-10-08') & \n",
    "#     (dd.timestamp <= '2016-10-10'), \n",
    "#     ['meter_reading_avg','timestamp']]\n",
    "\n",
    "# sns.lineplot(x='timestamp', y='meter_reading', data=d1)\n",
    "# sns.lineplot(x='timestamp', y='meter_reading_avg', data=d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Features to Include in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_feats = [\n",
    "        'square_feet',\n",
    "        'floor_count',\n",
    "        'air_temperature',\n",
    "        'dew_temperature',\n",
    "        'sea_level_pressure',\n",
    "        'wind_speed',\n",
    "        'precip_depth_1_hr',\n",
    "        'precip_depth_1_hr_nan', \n",
    "        'precip_depth_1_hr_isTrace',\n",
    "]\n",
    "\n",
    "cat_feats = [\n",
    "    'timestampDayofweek',\n",
    "    'primary_use',\n",
    "    'year_built',\n",
    "    'timestampMonth',\n",
    "#     'timestampWeek',\n",
    "    'timestampHour',\n",
    "    'weekend',\n",
    "    'site_id',\n",
    "    'building_id',\n",
    "    'meter'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train['timestamp'].is_monotonic_increasing:\n",
    "    raise Exception(\n",
    "        \"timestamp should be sorted in increasing order \"\n",
    "        \"for KFold validation to work properly\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNC\n",
    "train = train[cont_feats + cat_feats]\n",
    "print(\n",
    "    f\"Training on {train.shape[0]} records\",\n",
    "    f\"Number of features: {train.shape[1]}\",\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')  # CHANGED\n",
    "imputed_df = pd.DataFrame(imp.fit_transform(train))\n",
    "imputed_df.columns = train.columns\n",
    "train = imputed_df\n",
    "\n",
    "print(\"Saving train...\")\n",
    "feather.write_dataframe(train, MAIN / 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final dataset before training here so that \n",
    "# entire pipeline need not be run when switching between\n",
    "# validation and full training\n",
    "\n",
    "train = feather.read_dataframe(MAIN / 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold CV (Unshuffled)\n",
    "Variation of cv approach in \n",
    "\n",
    "https://www.kaggle.com/kimtaegwan/what-s-your-cv-method?scriptVersionId=22371767\n",
    "\n",
    "evaluated only on the second fold, since validation set for this are from a time period after the training set. Note disadvantage of current implementation of this approach: missing imputation by mean of a feature leaks into the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not train_full:\n",
    "\n",
    "    folds = 2\n",
    "\n",
    "    kf = model_selection.KFold(\n",
    "        n_splits=folds, shuffle=False, random_state=42)\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(train, y)):\n",
    "\n",
    "        # Skip first fold to avoid worst data leakage\n",
    "        # due to all training set time > validation set time\n",
    "        if fold_ == 0:\n",
    "            continue\n",
    "\n",
    "        print(fold_, trn_idx.shape, val_idx.shape)\n",
    "\n",
    "        # Note potential leakage here if missing imputation is done before \n",
    "        # before this cell\n",
    "        tr_x, tr_y = train.iloc[trn_idx], y[trn_idx]\n",
    "        vl_x, vl_y = train.iloc[val_idx], y[val_idx]\n",
    "\n",
    "        reg = lgb.LGBMRegressor(\n",
    "            learning_rate=0.05,\n",
    "            boosting=\"gbdt\",\n",
    "            n_estimators=3000,\n",
    "            feature_fraction=.7,\n",
    "            min_child_weight=3,\n",
    "            subsample=0.6,\n",
    "            colsample_bytree=.9,\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            n_jobs=8,\n",
    "            seed=27,\n",
    "            num_leaves=40\n",
    "        )\n",
    "\n",
    "        reg.fit(\n",
    "            tr_x, tr_y,\n",
    "            eval_set=[(vl_x, vl_y)],\n",
    "            early_stopping_rounds=200,\n",
    "            verbose=100,\n",
    "            categorical_feature=cat_feats\n",
    "        )\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full sample for submission\n",
    "if train_full:\n",
    "\n",
    "    print(\"Training on entire training dataset\")\n",
    "    # Number of estimators based on KFold CV results\n",
    "    n_estimators_cv = 500\n",
    "\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        learning_rate=0.05,\n",
    "        boosting=\"gbdt\",\n",
    "        n_estimators=n_estimators_cv,\n",
    "        feature_fraction=.7,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=.9,\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        n_jobs=8,\n",
    "        seed=27,\n",
    "        num_leaves=40\n",
    "    )\n",
    "    reg.fit(\n",
    "        train, y,\n",
    "        categorical_feature=cat_feats\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    reg.booster_.save_model(str(SUBMISSIONS_PATH / f'model_{submission_name}.txt'))\n",
    "    \n",
    "#     reg = LinearRegression()\n",
    "#     reg.fit(train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_submission:\n",
    "    # Evaluate on test set\n",
    "    test = pd.read_csv(MAIN / 'data' / 'test.csv')\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "    weather_test = pd.read_csv(MAIN / 'data' / 'weather_test.csv')\n",
    "    weather_test['timestamp'] = pd.to_datetime(weather_test['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Evaluation Set Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_submission:\n",
    "    tmp = rolling_stat(\n",
    "        weather_test, 'timestamp', ['site_id'], \n",
    "        cols_rol, period, np.mean\n",
    "    )\n",
    "    weather_test = weather_test.drop(cols_rol, 1)\n",
    "    weather_test = weather_test.merge(tmp, how='inner', on=['site_id', 'timestamp'])\n",
    "\n",
    "    # DNC\n",
    "    # Merge into training\n",
    "    test = test.merge(building_metadata, on='building_id', how='left')\n",
    "    test = test.merge(weather_test, on=['site_id', 'timestamp'], how='left')\n",
    "    \n",
    "    if sample:\n",
    "        test = test[test['building_id'].isin(randbuilding)]\n",
    "\n",
    "    print(\"Apply date operation...\")\n",
    "    add_datepart(\n",
    "        test, 'timestamp', datetimeformat=None,\n",
    "        drop=False, time=True, errors=\"raise\"\n",
    "    )\n",
    "    test[\"weekend\"] = test[\"timestamp\"].dt.weekday // 5\n",
    "\n",
    "    # Apply feature engineering to test set\n",
    "    print(\"Apply feature engineering and imputed values...\")\n",
    "    test,_ = precip_depth_1_hr_FE(test, m=precip_m)\n",
    "    test, _ = wind_direction_FE(test, m=wind_direction_m)\n",
    "    test['primary_use'] = le.transform(test['primary_use'])  # CHANGED\n",
    "\n",
    "    # Remove binding from namespace\n",
    "    # and force release of memory\n",
    "    del building_metadata, weather_train\n",
    "    gc.collect()\n",
    "\n",
    "    test = test[cont_feats + cat_feats + ['row_id']]\n",
    "    test['square_feet'] = np.log1p(test['square_feet'])\n",
    "    \n",
    "    # Apply missing imputation used in training\n",
    "    test_v = test.drop('row_id', 1).values\n",
    "    test_v = imp.transform(test_v)\n",
    "    test_v.shape\n",
    "    \n",
    "    # !!! Save and load prepared test set\n",
    "    feather.write_dataframe(test, MAIN / 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Submission Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_submission:\n",
    "    print(\"Generating submission\")\n",
    "    \n",
    "    test['meter_reading'] = np.expm1(reg.predict(test_v))\n",
    "\n",
    "    print(\"Clipping meter reading to zero...\")\n",
    "    # Save predictions as a column in a df\n",
    "    # Clip to a min of 0 and infinity (a_max is None)\n",
    "    test['meter_reading'] = np.clip(test['meter_reading'].values, 0, None)\n",
    "    \n",
    "    print(\"Copying subset of dataframe...\")\n",
    "    sample_submission = test[['row_id', 'meter_reading']].copy()\n",
    "\n",
    "    print(\"Recasting to float32 and rounding values...\")\n",
    "    sample_submission.loc[:,'meter_reading'] = (\n",
    "        sample_submission.loc[:, 'meter_reading'].\n",
    "        astype('float32').\n",
    "        round(2)\n",
    "    )\n",
    "    sample_submission.loc[:,'row_id'] = (\n",
    "        sample_submission.loc[:, 'row_id'].\n",
    "        astype('int32')\n",
    "    )\n",
    "\n",
    "    print(\"Saving csv...\")\n",
    "    sample_submission.to_csv(SUBMISSIONS_PATH / (submission_name + '.csv'), index=False)\n",
    "\n",
    "    print(sample_submission.shape)\n",
    "    print(sample_submission['meter_reading'].min(), sample_submission['meter_reading'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualize predictions\n",
    "Need\n",
    "\n",
    "* Trained model\n",
    "* training predictions with timestamp, building_id, and meter\n",
    "* test prediciton with timestamp, building_id, and meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
